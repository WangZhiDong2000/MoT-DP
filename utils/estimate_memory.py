#!/usr/bin/env python3
"""
GPUæ˜¾å­˜ä¼°è®¡å·¥å…·
æ ¹æ®æ¨¡å‹é…ç½®ä¼°è®¡GPUæ˜¾å­˜éœ€æ±‚
"""

import yaml
import sys
from pathlib import Path
from typing import Dict, Tuple

def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def estimate_model_memory(config: Dict) -> Dict:
    """
    è¯¦ç»†ä¼°è®¡æ¨¡å‹æ˜¾å­˜å ç”¨ (å•ä½: GB)
    åŸºäºå‚æ•°é‡å’Œä¸­é—´æ¿€æ´»
    """
    policy = config.get('policy', {})
    bev_encoder = config.get('bev_encoder', {})
    dataloader = config.get('dataloader', {})
    
    n_emb = policy.get('n_emb', 512)
    n_head = policy.get('n_head', 8)
    n_layer = policy.get('n_layer', 8)
    n_cond_layers = policy.get('n_cond_layers', 4)
    feature_dim = bev_encoder.get('feature_dim', 256)
    state_dim = bev_encoder.get('state_dim', 15)
    batch_size = dataloader.get('batch_size', 128)
    
    action_dim = policy.get('input_dim', 2)
    horizon = policy.get('horizon', 6)
    
    # 1. æ¨¡å‹å‚æ•° (å•ç²¾åº¦)
    # Transformerä¸»ç½‘ç»œ
    head_dim = n_emb // n_head
    params_per_layer = (
        4 * n_emb * n_emb +  # è‡ªæ³¨æ„åŠ›
        3 * n_emb * head_dim * n_head +  # QKVæŠ•å½±
        4 * n_emb * n_emb  # FFN
    )
    transformer_params = params_per_layer * n_layer / 1e9  # è½¬æ¢ä¸ºGB
    
    # æ¡ä»¶ç¼–ç å™¨
    cond_params = (
        feature_dim * n_emb +  # è¾“å…¥æŠ•å½±
        4 * n_emb * n_emb * n_cond_layers  # æ¡ä»¶ç¼–ç å±‚
    ) / 1e9
    
    # BEVç¼–ç å™¨ (ç²—ç•¥ä¼°è®¡)
    bev_params = 100 / 1e9  # å‡è®¾100Må‚æ•°
    
    total_params = transformer_params + cond_params + bev_params
    param_memory = total_params * 4 / 1024  # å•ç²¾åº¦ä¸º4å­—èŠ‚ï¼Œè½¬æ¢ä¸ºGB
    
    # 2. ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW: å‚æ•° + åŠ¨é‡ + æ–¹å·®)
    optimizer_memory = param_memory * 2 * 4 / 4  # 4å€å‚æ•°é‡
    
    # 3. æ¢¯åº¦å†…å­˜
    gradient_memory = param_memory
    
    # 4. æ¿€æ´»å‡½æ•°å†…å­˜ (ä¸»è¦è´¡çŒ®)
    # Batchä¸­é—´æ¿€æ´»ï¼šB * seq_len * n_emb * n_layer
    seq_len = horizon + 4 * 4  # action_horizon + obs_horizon * n_obs_steps
    activation_per_sample = seq_len * n_emb * n_layer * 4 / 1024 / 1024  # MB
    activations_memory = batch_size * activation_per_sample * 4 / 1024  # è½¬æ¢ä¸ºGB
    
    # 5. BEVç‰¹å¾å’Œè¾“å…¥æ•°æ®
    # BEV: B * feature_dim * 448 * 448
    bev_data_memory = batch_size * feature_dim * 448 * 448 * 4 / 1024 / 1024 / 1024
    
    # 6. å…¶ä»–å¼€é”€
    misc_memory = 2.0  # GPUé©±åŠ¨ã€ç¼“å†²ç­‰
    
    # è®¡ç®—æ€»æ˜¾å­˜
    total_memory = (
        param_memory + 
        optimizer_memory + 
        gradient_memory + 
        activations_memory + 
        bev_data_memory + 
        misc_memory
    )
    
    return {
        'param_memory': param_memory,
        'optimizer_memory': optimizer_memory,
        'gradient_memory': gradient_memory,
        'activation_memory': activations_memory,
        'bev_data_memory': bev_data_memory,
        'misc_memory': misc_memory,
        'total_memory': total_memory,
        'breakdown': {
            'parameters': f"{param_memory:.1f}GB",
            'optimizer': f"{optimizer_memory:.1f}GB",
            'gradients': f"{gradient_memory:.1f}GB",
            'activations': f"{activations_memory:.1f}GB",
            'bev_features': f"{bev_data_memory:.1f}GB",
            'misc': f"{misc_memory:.1f}GB",
        }
    }

def get_gpu_recommendations(total_memory: float) -> Tuple[list, str]:
    """æ ¹æ®æ˜¾å­˜éœ€æ±‚æ¨èGPU"""
    recommendations = []
    
    if total_memory < 8:
        recommendations = [
            'NVIDIA RTX 3050 (6GB)',
            'NVIDIA RTX 2060 (6GB)',
            'NVIDIA T4 (16GB)',
        ]
        level = 'ä½æ˜¾å­˜'
    elif total_memory < 12:
        recommendations = [
            'NVIDIA RTX 3060 (12GB)',
            'NVIDIA RTX 4060 (8GB)',
            'NVIDIA A10 (24GB)',
        ]
        level = 'ä¸­ç­‰æ˜¾å­˜'
    elif total_memory < 16:
        recommendations = [
            'NVIDIA RTX 3080 (10GB)',
            'NVIDIA RTX 4080 (12GB)',
            'NVIDIA A100 (40GB)',
        ]
        level = 'é«˜æ˜¾å­˜'
    elif total_memory < 24:
        recommendations = [
            'NVIDIA RTX 3090 (24GB)',
            'NVIDIA RTX 4090 (24GB)',
            'NVIDIA A100 (40GB)',
        ]
        level = 'é«˜æ˜¾å­˜'
    else:
        recommendations = [
            'NVIDIA A100 (40GB)',
            'NVIDIA H100 (80GB)',
            'NVIDIA A6000 (48GB)',
            'å¤šGPUè®­ç»ƒ',
        ]
        level = 'è¶…é«˜æ˜¾å­˜'
    
    return recommendations, level

def print_memory_report(config_path: str):
    """æ‰“å°è¯¦ç»†çš„æ˜¾å­˜æŠ¥å‘Š"""
    config = load_config(config_path)
    memory = estimate_model_memory(config)
    
    policy = config.get('policy', {})
    bev_encoder = config.get('bev_encoder', {})
    dataloader = config.get('dataloader', {})
    
    recommendations, level = get_gpu_recommendations(memory['total_memory'])
    
    print("\n" + "="*80)
    print("ğŸ“Š GPUæ˜¾å­˜ä¼°è®¡æŠ¥å‘Š".center(80))
    print("="*80)
    
    # é…ç½®æ€»ç»“
    print("\nğŸ“ æ¨¡å‹é…ç½®:")
    print(f"  â€¢ n_emb: {policy.get('n_emb', 512)}")
    print(f"  â€¢ n_head: {policy.get('n_head', 8)}")
    print(f"  â€¢ n_layer: {policy.get('n_layer', 8)}")
    print(f"  â€¢ n_cond_layers: {policy.get('n_cond_layers', 4)}")
    print(f"  â€¢ feature_dim: {bev_encoder.get('feature_dim', 256)}")
    print(f"  â€¢ batch_size: {dataloader.get('batch_size', 128)}")
    
    # æ˜¾å­˜åˆ†è§£
    print(f"\nğŸ“ˆ æ˜¾å­˜åˆ†è§£ (GPUæ˜¾å­˜å ç”¨):")
    total = memory['total_memory']
    for component, value_str in memory['breakdown'].items():
        value = float(value_str.replace('GB', ''))
        percentage = (value / total) * 100
        bar = "â–ˆ" * int(percentage / 5)
        print(f"  â€¢ {component:.<20} {value_str:>8} ({percentage:>5.1f}%) {bar}")
    
    print(f"\nğŸ¯ æ€»è®¡æ˜¾å­˜éœ€æ±‚: {total:.1f} GB")
    
    # GPUæ¨è
    print(f"\nğŸ’» æ¨èGPU (æ˜¾å­˜ç­‰çº§: {level}):")
    for i, gpu in enumerate(recommendations, 1):
        print(f"  {i}. {gpu}")
    
    # å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if total > 24:
        print(f"  âš ï¸  æ˜¾å­˜éœ€æ±‚è¾ƒå¤§ ({total:.1f}GB)")
        print(f"  ğŸ’¾ å¯é€‰æ–¹æ¡ˆ:")
        print(f"    â€¢ å‡å° batch_size (å½“å‰: {dataloader.get('batch_size', 128)})")
        print(f"    â€¢ å‡å° n_emb æˆ– n_layer")
        print(f"    â€¢ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")
        print(f"    â€¢ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    elif total > 16:
        print(f"  âœ… æ˜¾å­˜éœ€æ±‚é€‚ä¸­")
        print(f"  ğŸ’¡ å¯è€ƒè™‘ä½¿ç”¨RTX 3090æˆ–A100è¿›è¡Œè®­ç»ƒ")
    else:
        print(f"  âœ… æ˜¾å­˜éœ€æ±‚è¾ƒå°")
        print(f"  ğŸ’¡ å¯åœ¨æ¶ˆè´¹çº§GPUæˆ–æ™®é€šæœåŠ¡å™¨ä¸Šè®­ç»ƒ")
    
    print("\n" + "="*80)

def compare_configs_memory(config1_path: str, config2_path: str):
    """å¯¹æ¯”ä¸¤ä¸ªé…ç½®çš„æ˜¾å­˜éœ€æ±‚"""
    config1 = load_config(config1_path)
    config2 = load_config(config2_path)
    
    memory1 = estimate_model_memory(config1)
    memory2 = estimate_model_memory(config2)
    
    print("\n" + "="*80)
    print("ğŸ“Š é…ç½®æ˜¾å­˜å¯¹æ¯”".center(80))
    print("="*80)
    
    print(f"\né…ç½®1: {config1_path}")
    print(f"  æ€»æ˜¾å­˜: {memory1['total_memory']:.1f} GB")
    
    print(f"\né…ç½®2: {config2_path}")
    print(f"  æ€»æ˜¾å­˜: {memory2['total_memory']:.1f} GB")
    
    diff = memory2['total_memory'] - memory1['total_memory']
    percent = (diff / memory1['total_memory']) * 100 if memory1['total_memory'] > 0 else 0
    
    print(f"\nğŸ“Š å·®å¼‚:")
    if diff > 0:
        print(f"  ğŸ“ˆ é…ç½®2å¢åŠ : +{diff:.1f} GB ({percent:+.0f}%)")
    else:
        print(f"  ğŸ“‰ é…ç½®2å‡å°‘: {diff:.1f} GB ({percent:+.0f}%)")
    
    print("\n" + "="*80)

if __name__ == '__main__':
    if len(sys.argv) == 1:
        # é»˜è®¤æ£€æŸ¥å½“å‰é…ç½®
        config_path = '/home/wang/Project/MoT-DP/config/nuscenes.yaml'
        print_memory_report(config_path)
    elif len(sys.argv) == 2:
        if sys.argv[1] == '--default':
            config_path = '/home/wang/Project/MoT-DP/config/nuscenes.yaml'
            print_memory_report(config_path)
        else:
            print_memory_report(sys.argv[1])
    elif len(sys.argv) == 3:
        compare_configs_memory(sys.argv[1], sys.argv[2])
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python utils/estimate_memory.py                      # æ£€æŸ¥é»˜è®¤é…ç½®")
        print("  python utils/estimate_memory.py <config_path>        # æ£€æŸ¥æŒ‡å®šé…ç½®")
        print("  python utils/estimate_memory.py <cfg1> <cfg2>        # å¯¹æ¯”ä¸¤ä¸ªé…ç½®")
