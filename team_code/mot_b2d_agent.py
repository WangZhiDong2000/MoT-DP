import os
import sys
import json
import datetime
import pathlib
import time
import cv2
import carla
from collections import deque
import math
import yaml
import torch
import numpy as np
from PIL import Image
from torchvision import transforms as T
import imageio
import random
import sys
import numpy as np
from filterpy.kalman import MerweScaledSigmaPoints
from filterpy.kalman import UnscentedKalmanFilter as UKF

project_root = str(pathlib.Path(__file__).parent.parent.parent)
leaderboard_root = str(os.path.join(project_root, 'leaderboard'))
scenario_runner_root = str(os.path.join(project_root, 'scenario_runner'))
mot_dp_root = str(os.path.join(project_root, 'MoT-DP'))
carla_api_root = str(os.path.join(project_root.replace('Bench2Drive', 'carla'), 'PythonAPI', 'carla'))

for path in [project_root, leaderboard_root, scenario_runner_root, mot_dp_root, carla_api_root]:
    if os.path.exists(path) and path not in sys.path:
        sys.path.insert(0, path)

sys.path = [str(p) for p in sys.path]

from leaderboard.autoagents import autonomous_agent
from policy.diffusion_dit_carla_policy import DiffusionDiTCarlaPolicy
from team_code.simlingo.nav_planner import RoutePlanner, LateralPIDController  
from agents.navigation.local_planner import RoadOption
import team_code.simlingo.transfuser_utils as t_u  
from team_code.render import render, render_self_car, render_waypoints
from dataset.generate_lidar_bev_b2d import generate_lidar_bev_images
from scipy.optimize import fsolve
from scipy.interpolate import PchipInterpolator
import xml.etree.ElementTree as ET  
from srunner.scenariomanager.carla_data_provider import CarlaDataProvider  

# TransFuser backbone for DP features
from model.transfuser_extractor.backbone_extractor import TransFuserBackboneExtractor
from model.transfuser_extractor.config import GlobalConfig as TransfuserConfig
import model.transfuser_extractor.transfuser_utils as transfuser_t_u

# mot dependencies
project_root = str(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)
mot_dp_path = str(os.path.join(os.path.dirname(os.path.dirname(project_root)), 'MoT-DP'))
mot_path = str(os.path.join(mot_dp_path, 'mot'))
sys.path.append(mot_dp_path)
sys.path.append(mot_path)
sys.path = [str(p) for p in sys.path]

from transformers import HfArgumentParser
import json
from dataclasses import dataclass, field
from transformers.models.qwen3_vl.modeling_qwen3_vl import Qwen3VLVisionModel
from transformers.models.qwen3_vl.configuration_qwen3_vl import Qwen3VLVisionConfig
from PIL import Image
from safetensors.torch import load_file
import glob
from data.reasoning.data_utils import add_special_tokens
from mot.modeling.automotive import (
    AutoMoTConfig, AutoMoT,
    Qwen3VLTextConfig, Qwen3VLTextModel, Qwen3VLForConditionalGenerationMoT
)
from dataset.unified_carla_dataset import CARLAImageDataset
from policy.diffusion_dit_carla_policy import DiffusionDiTCarlaPolicy
from mot.evaluation.inference import InterleaveInferencer
from transformers import AutoTokenizer

# Import TransfuserData using importlib to avoid conflicts with mot/data
import importlib.util
team_code_transfuser_path = os.path.join(mot_dp_path, 'team_code', 'team_code_transfuser')
# Add team_code_transfuser to sys.path so its internal imports (like gaussian_target) work
if team_code_transfuser_path not in sys.path:
    sys.path.insert(0, team_code_transfuser_path)

_transfuser_data_spec = importlib.util.spec_from_file_location(
    "transfuser_data_module", 
    os.path.join(team_code_transfuser_path, "data.py")
)
_transfuser_data_module = importlib.util.module_from_spec(_transfuser_data_spec)
_transfuser_data_spec.loader.exec_module(_transfuser_data_module)
TransfuserData = _transfuser_data_module.CARLA_Data

# Import utility modules
from team_code.mot_utils import (
    ModelArguments, InferenceArguments,
    load_model_mot, build_cleaned_prompt_and_modes,
    parse_decision_sequence, split_prompt
)
from team_code.lidar_utils import lidar_to_ego_coordinate, algin_lidar
from team_code.ukf_utils import (
    bicycle_model_forward, measurement_function_hx,
    state_mean, measurement_mean,
    residual_state_x, residual_measurement_h
)
from team_code.display_interface import DisplayInterface

try:
    import pygame
except ImportError:
    raise RuntimeError("cannot import pygame, make sure pygame package is installed")

SAVE_PATH = os.environ.get('SAVE_PATH', None)
IS_BENCH2DRIVE = os.environ.get('IS_BENCH2DRIVE', None)
PLANNER_TYPE = os.environ.get('PLANNER_TYPE', None)
EARTH_RADIUS_EQUA = 6378137.0
USE_UKF = True  # Enable Unscented Kalman Filter for GPS/compass smoothing

# Entry point
def get_entry_point():
	return 'MOTAgent'

def create_carla_config(config_path=None):
    """Load CARLA configuration from YAML file."""
    if config_path is None:
        config_path = "/home/wang/Project/MoT-DP/config/pdm_local.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def load_best_model(checkpoint_path, config, device):
    """Load the DP (Diffusion Policy) best model checkpoint."""
    print(f"Loading best model from: {checkpoint_path}")
    if not hasattr(np, '_core'):
        sys.modules['numpy._core'] = np.core
        sys.modules['numpy._core._multiarray_umath'] = np.core._multiarray_umath
    
    # Load checkpoint to CPU first to save GPU memory
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

    policy = DiffusionDiTCarlaPolicy(config)
    policy.load_state_dict(checkpoint['model_state_dict'])
    
    # Print info before deleting checkpoint
    epoch = checkpoint.get('epoch', 'N/A')
    val_loss = checkpoint.get('val_loss', 'N/A')
    train_loss = checkpoint.get('train_loss', 'N/A')
    val_metrics = checkpoint.get('val_metrics', None)
    
    # Free checkpoint memory immediately
    del checkpoint
    import gc
    gc.collect()
    
    # Now move to device
    policy = policy.to(device)
    policy.eval()
    
    print(f"✓ Model loaded successfully!")
    print(f"  - Epoch: {epoch}")
    if isinstance(val_loss, float):
        print(f"  - Validation Loss: {val_loss:.4f}")
    if isinstance(train_loss, float):
        print(f"  - Training Loss: {train_loss:.4f}")
    
    if val_metrics:
        print(f"  - Validation Metrics:")
        for key, value in val_metrics.items():
            if isinstance(value, (int, float)):
                print(f"    {key}: {value:.4f}")

    return policy


class MOTAgent(autonomous_agent.AutonomousAgent):
	def setup(self, path_to_conf_file):
		self.track = autonomous_agent.Track.SENSORS
		if IS_BENCH2DRIVE:
			self.save_name = path_to_conf_file.split('+')[-1]
			self.config_path = path_to_conf_file.split('+')[0]
		else:
			now = datetime.datetime.now()
			self.config_path = path_to_conf_file
			self.save_name = '_'.join(map(lambda x: '%02d' % x, (now.month, now.day, now.hour, now.minute, now.second)))
		self.step = -1
		self.wall_start = time.time()
		self.initialized = False

		import gc
		
		# Load diffusion policy first (smaller model)
		print("Loading diffusion policy...")
		self.config = create_carla_config()
		device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
		checkpoint_base_path = self.config.get('training', {}).get('checkpoint_dir', "/home/wang/Project/MoT-DP/checkpoints/carla_dit_best")
		checkpoint_path = os.path.join(checkpoint_base_path, "dit_policy_best_epoch129.pt")
		self.net = load_best_model(checkpoint_path, self.config, device)
		self.net = self.net.to(torch.bfloat16)
		if hasattr(self.net, 'obs_encoder'):
			self.net.obs_encoder = self.net.obs_encoder.to(torch.bfloat16)
		print("✓ Diffusion policy loaded (bfloat16).")
		
		# Aggressive memory cleanup before loading MoT model
		gc.collect()
		if torch.cuda.is_available():
			torch.cuda.empty_cache()
			torch.cuda.synchronize()
		
		# Print GPU memory status
		if torch.cuda.is_available():
			allocated = torch.cuda.memory_allocated() / 1024**3
			reserved = torch.cuda.memory_reserved() / 1024**3
			print(f"[GPU Memory] After DP: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")

		# Load MoT model
		print("Loading MoT model...")
		parser = HfArgumentParser((ModelArguments, InferenceArguments))
		model_args, inference_args = parser.parse_args_into_dataclasses(args=[])
		self.inference_args = inference_args  
		self.AutoMoT = load_model_mot(device)
		tokenizer = AutoTokenizer.from_pretrained(model_args.qwen3vl_path)
		tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)
		self.AutoMoT.language_model.tokenizer = tokenizer
		self.inferencer = InterleaveInferencer(
        model=self.AutoMoT,
        vae_model=None,
        tokenizer=tokenizer,
        vae_transform=None,
        vit_transform=None,  # Not used for Qwen3VL, handled internally by model
        new_token_ids=new_token_ids,
        max_num_tokens=inference_args.max_num_tokens,
        visual_gen=True,  # Enable visual generation to initialize query tokens
        visual_und=True,  # Enable visual understanding
    	)
		print("✓ MoT model loaded.")

		# ========== Load TransFuser Backbone for DP features ==========
		print("Loading TransFuser backbone for DP features...")
		transfuser_config_path = "/home/wang/Project/carla_garage/leaderboard/leaderboard/pretrained_models/all_towns"
		self.transfuser_backbone = TransFuserBackboneExtractor(
			config_path=transfuser_config_path,
			device='cuda:0'
		)
		# Backbone is already frozen in TransFuserBackboneExtractor
		self.transfuser_backbone.eval()
		# Convert to bfloat16 to match DP model precision
		self.transfuser_backbone = self.transfuser_backbone.to(torch.bfloat16)
		# Get transfuser config for lidar processing
		self.transfuser_config = self.transfuser_backbone.config
		# Initialize TransfuserData for lidar histogram conversion
		self.transfuser_data = TransfuserData(root=[], config=self.transfuser_config, shared_dict=None)
		print("✓ TransFuser backbone loaded, frozen, and converted to bfloat16.")
		
		# Initialize transfuser lidar buffer for temporal alignment
		self.transfuser_lidar_buffer = deque(maxlen=self.transfuser_config.lidar_seq_len * self.transfuser_config.data_save_freq)
		self.transfuser_lidar_last = None
		self.transfuser_state_log = deque(maxlen=max((self.transfuser_config.lidar_seq_len * self.transfuser_config.data_save_freq), 2))
		
		# Print GPU memory status
		gc.collect()
		if torch.cuda.is_available():
			torch.cuda.empty_cache()
			allocated = torch.cuda.memory_allocated() / 1024**3
			reserved = torch.cuda.memory_reserved() / 1024**3
			print(f"[GPU Memory] After TransFuser: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")

		# route_pred有20个稀疏点(~20m, 1m间距)，插值后约200点(0.1m间距)
		# inference_mode=False: lookahead范围24-105个点，对应2.4m-10.5m前方
		# 使用自定义的LateralPIDController，增加低速时的最小前视距离
		# 默认 speed_offset=1.915, 低速时 lookahead = 0.9755*speed_kmh + 1.915，最小24
		# 增加 speed_offset 和最小lookahead，让低速时看得更远，有助于直行时回正
		self.turn_controller = LateralPIDController(
			inference_mode=False, 
			k_p=3.118,  # 增加P增益，提高回正响应（默认3.118）
			speed_offset=1.195,  # 增加offset，低速时看更远（默认1.915）
			default_lookahead=24  # 增加最小前视距离到4m（默认24=2.4m）
		)
		self.speed_controller = t_u.PIDController(k_p=1.75, k_i=1.0, k_d=2.0, n=20)  
		
		# Control config 
		self.carla_fps = 20
		self.wp_dilation = 1
		self.data_save_freq = 5
		self.brake_speed = 0.4
		self.brake_ratio = 1.1
		self.clip_delta = 1.0
		self.clip_throttle = 1.0
		self.stuck_threshold = 300 #800
		self.stuck_helper_threshold = 100
		self.creep_duration = 15
		self.creep_throttle = 0.4
		
		# Stuck detection
		self.stuck_detector = 0
		self.stuck_helper = 0
		self.force_move = 0

		self.steer_step = 0
		self.last_moving_status = 0
		self.last_moving_step = -1
		self.last_steers = 0
		
		self.takeover = False
		self.stop_time = 0
		self.takeover_time = 0
		self.save_path = None
		self._im_transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])
		self.lat_ref, self.lon_ref = 42.0, 2.0
		control = carla.VehicleControl()
		control.steer = 0.0
		control.throttle = 0.0
		control.brake = 0.0	
		self.prev_control = control
		self.control = control  # Store control for UKF prediction
		
		# Initialize Unscented Kalman Filter 
		self.carla_frame_rate = 1.0 / 20.0  # CARLA frame rate
		if USE_UKF:
			self.points = MerweScaledSigmaPoints(n=4, alpha=0.00001, beta=2, kappa=0, subtract=residual_state_x)
			self.ukf = UKF(dim_x=4,
						   dim_z=4,
						   fx=bicycle_model_forward,
						   hx=measurement_function_hx,
						   dt=self.carla_frame_rate,
						   points=self.points,
						   x_mean_fn=state_mean,
						   z_mean_fn=measurement_mean,
						   residual_x=residual_state_x,
						   residual_z=residual_measurement_h)
			# State noise, same as measurement because we initialize with the first measurement later
			self.ukf.P = np.diag([0.5, 0.5, 0.000001, 0.000001])
			# Measurement noise
			self.ukf.R = np.diag([0.5, 0.5, 0.000000000000001, 0.000000000000001])
			self.ukf.Q = np.diag([0.0001, 0.0001, 0.001, 0.001])  # Model noise
			# Used to set the filter state equal the first measurement
			self.filter_initialized = False
			# Stores the last filtered positions of the ego vehicle
			self.state_log = deque(maxlen=20)

		if SAVE_PATH is not None:
			now = datetime.datetime.now()
			string = self.save_name
			print (string)

		self.save_path = pathlib.Path(os.environ['SAVE_PATH']) / string
		self.save_path.mkdir(parents=True, exist_ok=False)

		(self.save_path / 'rgb_front').mkdir()
		(self.save_path / 'meta').mkdir()
		(self.save_path / 'bev').mkdir()
		(self.save_path / 'lidar_bev').mkdir()
		
		# Initialize lidar buffer for combining two frames
		self.lidar_buffer = deque(maxlen=2)
		self.lidar_step_counter = 0
		self.last_ego_transform = None
		self.last_lidar = None
		
		obs_horizon = self.config.get('obs_horizon', 4)   
		self.obs_horizon = obs_horizon
		self.lidar_bev_history = deque(maxlen=obs_horizon*10) 
		self.rgb_history = deque(maxlen=obs_horizon*10)
		self.speed_history = deque(maxlen=obs_horizon*10)
		self.theta_history = deque(maxlen=obs_horizon*10)		# tg = tick_data['target_point']
		# tg[1] = 0.05
		# target_point = torch.from_numpy(tg).unsqueeze(0).float().to('cuda', dtype=torch.float32)e(maxlen=obs_horizon*10)
		self.throttle_history = deque(maxlen=obs_horizon*10)
		self.next_command_history = deque(maxlen=obs_horizon*10)
		self.target_point_history = deque(maxlen=obs_horizon*10)
		self.next_target_point_history = deque(maxlen=obs_horizon*10)  # For DP model input
		self.waypoint_history = deque(maxlen=obs_horizon*10)
		self.throttle_history = deque(maxlen=obs_horizon*10) 
		self.brake_history = deque(maxlen=obs_horizon*10) 
		self.obs_accumulate_counter = 0
		
		# Store predicted trajectory for BEV visualization
		self.last_pred_traj = None  # Store the last predicted trajectory (in ego frame)
		self.last_dp_pred_traj = None  # Store the last DP refined trajectory (in ego frame)
		self.last_target_point = None  # Store the last target point (in ego frame)
		self.last_next_target_point = None  # Store the last next target point (in ego frame)
		self.last_route_pred = None  # Store the last route prediction (20 waypoints for lateral control)

	def _init(self):
		# Use _global_plan_world_coord directly (already in CARLA coordinates)
		# This avoids the GPS-to-CARLA conversion which can fail when fsolve doesn't converge
		# Get lat_ref/lon_ref from CARLA map directly
		try:
			world_map = CarlaDataProvider.get_map()
			xodr = world_map.to_opendrive()
			tree = ET.ElementTree(ET.fromstring(xodr))
			
			# Default values if not found in OpenDRIVE
			self.lat_ref = 42.0
			self.lon_ref = 2.0
			
			for opendrive in tree.iter('OpenDRIVE'):
				for header in opendrive.iter('header'):
					for georef in header.iter('geoReference'):
						if georef.text:
							str_list = georef.text.split(' ')
							for item in str_list:
								if '+lat_0' in item:
									self.lat_ref = float(item.split('=')[1])
								if '+lon_0' in item:
									self.lon_ref = float(item.split('=')[1])
		except Exception as e:
			# Fallback: try fsolve (might not converge)
			try:
				locx, locy = self._global_plan_world_coord[0][0].location.x, self._global_plan_world_coord[0][0].location.y
				lon, lat = self._global_plan[0][0]['lon'], self._global_plan[0][0]['lat']
				earth_radius_equa = 6378137.0
				def equations(variables):
					x, y = variables
					eq1 = (lon * math.cos(x * math.pi / 180.0) - (locx * x * 180.0) / (math.pi * earth_radius_equa)
								 - math.cos(x * math.pi / 180.0) * y)
					eq2 = (math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * earth_radius_equa
								 * math.cos(x * math.pi / 180.0) + locy - math.cos(x * math.pi / 180.0) * earth_radius_equa
								 * math.log(math.tan((90.0 + x) * math.pi / 360.0)))
					return [eq1, eq2]
				initial_guess = [0.0, 0.0]
				solution = fsolve(equations, initial_guess)
				self.lat_ref, self.lon_ref = solution[0], solution[1]
			except Exception as e2:
				self.lat_ref, self.lon_ref = 0.0, 0.0
		

		self.route_planner_min_distance = 7.5
		self.route_planner_max_distance = 50.0
		self._route_planner = RoutePlanner(self.route_planner_min_distance, self.route_planner_max_distance,
										   self.lat_ref, self.lon_ref)
		
		if len(self._global_plan_world_coord) > 0:
			first_wp = self._global_plan_world_coord[0]
		
		# Use _global_plan_world_coord with gps=False (recommended, GPS is deprecated in nav_planner.py)
		self._route_planner.set_route(self._global_plan_world_coord, gps=False)
		
				
		# Initialize command tracking 
		self.commands = deque(maxlen=2)
		self.commands.append(4)
		self.commands.append(4)
		self.target_point_prev = [1e5, 1e5, 1e5]
		self.last_command = -1
		self.last_command_tmp = -1
		
		self.initialized = True
		self.metric_info = {}
		self._hic = DisplayInterface()

	def _build_obs_dict(self, tick_data, lidar, rgb_front, speed, theta, target_point, next_target_point, cmd_one_hot, waypoint):
		"""
		Build observation dictionaries from historical data.
		
		Args:
			target_point: Current target point in ego frame
			next_target_point: Next target point in ego frame (used together with target_point for DP model)
		
		Returns:
			lidar_stacked: (1, obs_horizon, C, H, W) stacked lidar BEV images
			ego_status_stacked: (1, obs_horizon, 14) concatenated ego status features
			                    Order: speed(1) + theta(1) + cmd(6) + target_point(2) + next_target_point(2) + waypoint_relative(2)
			rgb_stacked: (1, 5, C, H, W) stacked RGB images
		"""
		# Build obs_dict from historical observations
		lidar_history_list = list(self.lidar_bev_history)
		speed_history_list = list(self.speed_history)
		target_point_history_list = list(self.target_point_history)
		next_target_point_history_list = list(self.next_target_point_history)  # For DP model input
		cmd_history_list = list(self.next_command_history)
		theta_history_list = list(self.theta_history)
		throttle_history_list = list(self.throttle_history)
		brake_history_list = list(self.brake_history)
		rgb_history_list = list(self.rgb_history)
		waypoint_history_list = list(self.waypoint_history)
		
		lidar_list = [lidar_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(lidar_history_list)]
		speed_list = [speed_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(speed_history_list)]
		target_point_list = [target_point_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(target_point_history_list)]
		next_target_point_list = [next_target_point_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(next_target_point_history_list)]  # For DP model input
		cmd_list = [cmd_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(cmd_history_list)]
		theta_list = [theta_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(theta_history_list)]
		throttle_list = [throttle_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(throttle_history_list)]
		brake_list = [brake_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(brake_history_list)]
		waypoint_list = [waypoint_history_list[-1 - i*10] for i in range(self.obs_horizon) if -1 - i*10 >= -len(waypoint_history_list)]
		
		# Reverse to get chronological order (oldest to newest)
		lidar_list = lidar_list[::-1]
		speed_list = speed_list[::-1]
		target_point_list = target_point_list[::-1]
		next_target_point_list = next_target_point_list[::-1]  # For DP model input
		cmd_list = cmd_list[::-1]
		theta_list = theta_list[::-1]
		throttle_list = throttle_list[::-1]
		brake_list = brake_list[::-1]
		waypoint_list = waypoint_list[::-1]
		
		# sample every 5 for rgb from the end (t0, t-5, t-10, t-15)
		rgb_list = [rgb_history_list[-1 - i*5] for i in range(4) if -1 - i*5 >= -len(rgb_history_list)]
		rgb_list = rgb_list[::-1]  
		
		# Stack along time dimension
		lidar_stacked = torch.stack(lidar_list, dim=0).unsqueeze(0)  # (1, obs_horizon, C, H, W)
		speed_stacked = torch.cat(speed_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 1)
		theta_stacked = torch.cat(theta_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 1)
		cmd_stacked = torch.cat(cmd_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 6)
		target_point_stacked = torch.cat(target_point_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 2)
		waypoint_stacked = torch.stack(waypoint_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 2)
		rgb_stacked = torch.stack(rgb_list, dim=0).unsqueeze(0)  # (1, 5, C, H, W)
		

		current_pos = tick_data['gps']  
		current_theta = tick_data['theta']  # This is already preprocessed: compass - 90°
		
		# Build rotation matrix for world-to-ego transformation
		# R = [[cos, -sin], [sin, cos]] is the ego-to-world rotation
		# R.T = [[cos, sin], [-sin, cos]] is the world-to-ego rotation
		# This matches inverse_conversion_2d: R.T @ (point - translation)
		cos_theta = np.cos(current_theta)
		sin_theta = np.sin(current_theta)
		current_R = np.array([
			[cos_theta, sin_theta],
			[-sin_theta, cos_theta]
		])  # This is R.T, for world-to-ego transformation
		
		# Transform each historical waypoint to current frame
		waypoint_relative_list = []
		for i in range(self.obs_horizon):
			past_waypoint = waypoint_stacked[0, i].cpu().numpy()  # [x, y] in global coordinates
			# Transform from world to ego frame: R.T @ (past - current)
			relative_waypoint = current_R @ (past_waypoint - current_pos)
			waypoint_relative_list.append(torch.from_numpy(relative_waypoint).float().to('cuda'))
		
		waypoint_relative_stacked = torch.stack(waypoint_relative_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 2)
		
		# Transform target_point history to current ego frame (same logic as next_target_point)
		target_point_transformed_list = []
		for i in range(self.obs_horizon):
			past_world_pos = waypoint_list[i].cpu().numpy()  # [x, y] in world coordinates
			past_theta = theta_list[i].squeeze().cpu().item()  # theta value for past frame (already preprocessed)
			target_in_past_ego = target_point_list[i].squeeze().cpu().numpy()  # [x, y] in past ego frame
			
			# Build past ego-to-world rotation matrix
			cos_past = np.cos(past_theta)
			sin_past = np.sin(past_theta)
			past_R_ego_to_world = np.array([
				[cos_past, -sin_past],
				[sin_past, cos_past]
			])  # This is R, for ego-to-world transformation
			
			# Step 1: Transform target_point from past ego frame to world frame
			target_world = past_R_ego_to_world @ target_in_past_ego + past_world_pos
			
			# Step 2: Transform from world frame to current ego frame
			target_in_current_ego = current_R @ (target_world - current_pos)
			
			target_point_transformed_list.append(torch.from_numpy(target_in_current_ego).float().to('cuda'))
		
		target_point_transformed_stacked = torch.stack(target_point_transformed_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 2)
		
		# Transform next_target_point history to current ego frame
		next_target_point_transformed_list = []
		for i in range(self.obs_horizon):
			past_world_pos = waypoint_list[i].cpu().numpy()  # [x, y] in world coordinates
			past_theta = theta_list[i].squeeze().cpu().item()  # theta value for past frame (already preprocessed)
			next_target_in_past_ego = next_target_point_list[i].squeeze().cpu().numpy()  # [x, y] in past ego frame
			
			# Build past ego-to-world rotation matrix
			cos_past = np.cos(past_theta)
			sin_past = np.sin(past_theta)
			past_R_ego_to_world = np.array([
				[cos_past, -sin_past],
				[sin_past, cos_past]
			])  # This is R, for ego-to-world transformation
			
			# Step 1: Transform next_target_point from past ego frame to world frame
			next_target_world = past_R_ego_to_world @ next_target_in_past_ego + past_world_pos
			
			# Step 2: Transform from world frame to current ego frame
			next_target_in_current_ego = current_R @ (next_target_world - current_pos)
			
			next_target_point_transformed_list.append(torch.from_numpy(next_target_in_current_ego).float().to('cuda'))
		
		next_target_point_transformed_stacked = torch.stack(next_target_point_transformed_list, dim=0).unsqueeze(0)  # (1, obs_horizon, 2)
		
		
		# Concatenate all ego status features following unified_carla_dataset order:
		# speed(1) + theta(1) + cmd(6) + target_point(2) + next_target_point(2) + waypoint_relative(2) = 14
		ego_status_stacked = torch.cat([
			speed_stacked,                            # (1, obs_horizon, 1)
			theta_stacked,                            # (1, obs_horizon, 1)
			cmd_stacked,                              # (1, obs_horizon, 6)
			target_point_transformed_stacked,         # (1, obs_horizon, 2) - target_points transformed to current ego frame
			next_target_point_transformed_stacked,    # (1, obs_horizon, 2) - next_target_points transformed to current ego frame
			waypoint_relative_stacked                 # (1, obs_horizon, 2) - ego positions in current frame
		], dim=-1)  # Concatenate along feature dimension
		
		return lidar_stacked, ego_status_stacked, rgb_stacked


	def sensors(self):
		sensors =  [
				{
					'type': 'sensor.camera.rgb',
					'x': -1.50, 'y': 0.0, 'z': 2.0,
					'roll': 0.0, 'pitch': 0.0, 'yaw': 0.0,
					'width': 1024, 'height': 512, 'fov': 110,
					'id': 'CAM_FRONT'
					},
				# lidar
				{
          			'type': 'sensor.lidar.ray_cast',
          			'x': 0.0, 'y': 0.0, 'z': 2.5,
          			'roll': 0.0, 'pitch': 0.0, 'yaw': -90.0,
          			'id': 'LIDAR'
      				},
				# imu
				{
					'type': 'sensor.other.imu',
					'x': 0.0, 'y': 0.0, 'z': 0.0,
					'roll': 0.0, 'pitch': 0.0, 'yaw': 0.0,
					'sensor_tick': 0.05,
					'id': 'IMU'
					},
				# gps
				{
					'type': 'sensor.other.gnss',
					'x': 0.0, 'y': 0.0, 'z': 0.0,
					'roll': 0.0, 'pitch': 0.0, 'yaw': 0.0,
					'sensor_tick': 0.01,
					'id': 'GPS'
					},
				# speed
				{
					'type': 'sensor.speedometer',
					'reading_frequency': 20,
					'id': 'SPEED'
					},
				]
		
		if IS_BENCH2DRIVE:
			sensors += [
					{	
						'type': 'sensor.camera.rgb',
						'x': 0.0, 'y': 0.0, 'z': 50.0,
						'roll': 0.0, 'pitch': -90.0, 'yaw': 0.0,
						'width': 512, 'height': 512, 'fov': 5 * 10.0,
						'id': 'bev'
					}]
		return sensors

	def tick(self, input_data):
		self.step += 1
		rgb_front = cv2.cvtColor(input_data['CAM_FRONT'][1][:, :, :3], cv2.COLOR_BGR2RGB)
		lidar_ego = lidar_to_ego_coordinate(input_data['LIDAR'])
		
		gps_full = input_data['GPS'][1]  # [lat, lon, altitude]
		gps_pos = self._route_planner.convert_gps_to_carla(gps_full)
		
		# Handle compass NaN 
		compass_raw = input_data['IMU'][1][-1]
		if math.isnan(compass_raw):
			print("compass sends nan!!!")
			compass_raw = 0.0
		
		# Preprocess compass to CARLA coordinate system
		compass = t_u.preprocess_compass(compass_raw)
		
		# Get speed for UKF
		speed = input_data['SPEED'][1]['speed']
		
		# Apply Unscented Kalman Filter 
		if USE_UKF:
			if not self.filter_initialized:
				self.ukf.x = np.array([gps_pos[0], gps_pos[1], t_u.normalize_angle(compass), speed])
				self.filter_initialized = True

			self.ukf.predict(steer=self.control.steer, throttle=self.control.throttle, brake=self.control.brake)
			self.ukf.update(np.array([gps_pos[0], gps_pos[1], t_u.normalize_angle(compass), speed]))
			filtered_state = self.ukf.x

			self.state_log.append(filtered_state)
			gps_filtered = filtered_state[0:2]
			compass_filtered = filtered_state[2]
		else:
			gps_filtered = np.array([gps_pos[0], gps_pos[1]])
			compass_filtered = compass
		
		# Combine two frames of lidar data using algin_lidar
		# Use filtered GPS for lidar alignment
		if self.last_lidar is not None and self.last_ego_transform is not None:
			# Calculate relative transformation between current and last frame
			current_pos = np.array([gps_filtered[0], gps_filtered[1], 0.0])
			last_pos = np.array([self.last_ego_transform['gps'][0], self.last_ego_transform['gps'][1], 0.0])
			relative_translation = current_pos - last_pos
			
			# Calculate relative rotation using filtered compass
			current_yaw = compass_filtered
			last_yaw = self.last_ego_transform['compass']
			relative_rotation = current_yaw - last_yaw
			
			# Rotate difference vector from global to local coordinate system
			rotation_matrix = np.array([[np.cos(current_yaw), -np.sin(current_yaw), 0.0],
										[np.sin(current_yaw), np.cos(current_yaw), 0.0], 
										[0.0, 0.0, 1.0]])
			relative_translation_local = rotation_matrix.T @ relative_translation
			
			# Align the last lidar to current coordinate system
			lidar_last = algin_lidar(self.last_lidar, relative_translation_local, relative_rotation)
			# Combine lidar frames
			lidar_combined = np.concatenate((lidar_ego, lidar_last), axis=0)
		else:
			lidar_combined = lidar_ego
		
		# Store current frame for next iteration (use filtered values)
		self.last_lidar = lidar_ego
		self.last_ego_transform = {'gps': gps_filtered, 'compass': compass_filtered}
		
		# Generate lidar BEV image from combined lidar data
		lidar_bev_img = generate_lidar_bev_images(
			np.copy(lidar_combined), 
			saving_name=None, 
			img_height=448, 
			img_width=448
		)
		# Convert BEV image to tensor format for interfuser_bev_encoder backbone
		lidar_bev_tensor = torch.from_numpy(lidar_bev_img).permute(2, 0, 1).float() / 255.0
		
		# ========== TransFuser style processing for DP features ==========
		# Process RGB for TransFuser (same as team_code_transfuser/sensor_agent.py)
		transfuser_rgb = input_data['CAM_FRONT'][1][:, :, :3]
		# Add jpg artifacts at test time, because the training data was saved as jpg
		_, compressed_image = cv2.imencode('.jpg', transfuser_rgb)
		transfuser_rgb = cv2.imdecode(compressed_image, cv2.IMREAD_UNCHANGED)
		transfuser_rgb = cv2.cvtColor(transfuser_rgb, cv2.COLOR_BGR2RGB)
		# Crop RGB image (same as transfuser training)
		transfuser_rgb = transfuser_t_u.crop_array(self.transfuser_config, transfuser_rgb)
		# Convert to PyTorch format (C, H, W) and batch
		transfuser_rgb = np.transpose(transfuser_rgb, (2, 0, 1))
		transfuser_rgb_tensor = torch.from_numpy(transfuser_rgb).float().unsqueeze(0).to('cuda')
		
		# Process LiDAR for TransFuser (same as team_code_transfuser/sensor_agent.py)
		transfuser_lidar = transfuser_t_u.lidar_to_ego_coordinate(self.transfuser_config, input_data['LIDAR'])
		
		# Store state for lidar alignment
		self.transfuser_state_log.append([gps_filtered[0], gps_filtered[1], compass_filtered, speed])
		
		# We only get half a LiDAR at every time step. Align the last half into the current frame.
		if self.transfuser_lidar_last is not None and len(self.transfuser_state_log) >= 2:
			ego_x = self.transfuser_state_log[-1][0]
			ego_y = self.transfuser_state_log[-1][1]
			ego_theta = self.transfuser_state_log[-1][2]
			
			ego_x_last = self.transfuser_state_log[-2][0]
			ego_y_last = self.transfuser_state_log[-2][1]
			ego_theta_last = self.transfuser_state_log[-2][2]
			
			transfuser_lidar_last_aligned = self._align_lidar_transfuser(
				self.transfuser_lidar_last, 
				ego_x_last, ego_y_last, ego_theta_last,
				ego_x, ego_y, ego_theta
			)
			transfuser_lidar_full = np.concatenate((transfuser_lidar, transfuser_lidar_last_aligned), axis=0)
		else:
			transfuser_lidar_full = transfuser_lidar
		
		self.transfuser_lidar_last = transfuser_lidar.copy()
		self.transfuser_lidar_buffer.append(transfuser_lidar_full)
		
		# Convert to histogram BEV (same as sensor_agent.py)
		transfuser_lidar_bev = self.transfuser_data.lidar_to_histogram_features(
			transfuser_lidar_full,
			use_ground_plane=self.transfuser_config.use_ground_plane
		)
		transfuser_lidar_bev_tensor = torch.from_numpy(transfuser_lidar_bev).float().unsqueeze(0).to('cuda')
		
		# Process other sensors
		bev = cv2.cvtColor(input_data['bev'][1][:, :, :3], cv2.COLOR_BGR2RGB)
		
		result = {
				'rgb_front': rgb_front,
				'lidar_bev': lidar_bev_tensor,
				'gps': gps_filtered,  # Use UKF filtered CARLA coordinates
				'speed': speed,
				'compass': compass_filtered,  # Use UKF filtered compass
				'bev': bev,
				# TransFuser processed data for DP
				'transfuser_rgb': transfuser_rgb_tensor,  # (1, 3, H, W) on GPU
				'transfuser_lidar_bev': transfuser_lidar_bev_tensor,  # (1, C, H, W) on GPU
				}
		
		waypoint_route = self._route_planner.run_step(np.append(result['gps'], gps_pos[2]))
		

		
		if len(waypoint_route) > 2:
			target_point, far_command = waypoint_route[1]
			next_target_point, next_far_command = waypoint_route[2]
		elif len(waypoint_route) > 1:
			target_point, far_command = waypoint_route[1]
			# Only target_point available, generate virtual next_target_point
			# Extend 50m along the direction from ego to target_point (in world frame)
			ego_pos = result['gps'][:2]
			direction = target_point[:2] - ego_pos
			dist = np.linalg.norm(direction)
			if dist > 1e-3:
				direction_normalized = direction / dist
			else:
				# If target_point is too close, use forward direction based on compass
				direction_normalized = np.array([np.cos(result['compass']), np.sin(result['compass'])])
			next_target_point = target_point[:2] + direction_normalized * 50.0
			next_far_command = far_command
		elif len(waypoint_route) > 0:
			target_point, far_command = waypoint_route[0]
			# Only one waypoint available, generate virtual next_target_point
			# Extend 50m along the direction from ego to target_point (in world frame)
			ego_pos = result['gps'][:2]
			direction = target_point[:2] - ego_pos
			dist = np.linalg.norm(direction)
			if dist > 1e-3:
				direction_normalized = direction / dist
			else:
				# If target_point is too close, use forward direction based on compass
				direction_normalized = np.array([np.cos(result['compass']), np.sin(result['compass'])])
			next_target_point = target_point[:2] + direction_normalized * 50.0
			next_far_command = far_command
		else:
			# waypoint_route 为空的极端情况，使用当前位置
			target_point, far_command = (result['gps'][:2], RoadOption.LANEFOLLOW)
			# Generate virtual next_target_point 50m ahead in ego's forward direction
			direction_normalized = np.array([np.cos(result['compass']), np.sin(result['compass'])])
			next_target_point = result['gps'][:2] + direction_normalized * 50.0
			next_far_command = RoadOption.LANEFOLLOW

		if self.last_command_tmp != far_command:
			self.last_command = self.last_command_tmp
		self.last_command_tmp = far_command
		
		if hasattr(target_point, '__iter__') and len(target_point) >= 2:
			if (target_point[:2] != self.target_point_prev[:2]).any() if isinstance(target_point, np.ndarray) else (list(target_point[:2]) != list(self.target_point_prev[:2])):
				self.target_point_prev = target_point
				self.commands.append(far_command.value)
		
		result['next_command'] = self.commands[-2]
		ego_target_point = t_u.inverse_conversion_2d(target_point[:2], result['gps'], result['compass']) #result['compass'])
		ego_next_target_point = t_u.inverse_conversion_2d(next_target_point[:2], result['gps'], result['compass']) #result['compass'])

		# Debug: print target point transformation
		if self.step <= 5:
			print(f"  target_point (world): {target_point[:2]}")
			print(f"  ego position (gps): {result['gps']}")
			print(f"  compass (heading): {result['compass']:.4f} rad ({np.rad2deg(result['compass']):.2f} deg)")
			print(f"  ego_target_point: {ego_target_point}")
		
		result['target_point'] = ego_target_point  # numpy array (2,)
		result['next_target_point'] = ego_next_target_point  # numpy array (2,)
		result['theta'] = compass_filtered  # Use UKF filtered compass

		return result

	def _align_lidar_transfuser(self, lidar, x, y, orientation, x_target, y_target, orientation_target):
		"""
		Align lidar from past frame to current frame (same as sensor_agent.py).
		
		Args:
			lidar: numpy LiDAR point cloud (N, 3)
			x, y, orientation: past frame ego pose
			x_target, y_target, orientation_target: current frame ego pose
			
		Returns:
			aligned_lidar: numpy LiDAR point cloud in current frame coordinates
		"""
		pos_diff = np.array([x_target, y_target, 0.0]) - np.array([x, y, 0.0])
		rot_diff = transfuser_t_u.normalize_angle(orientation_target - orientation)
		
		# Rotate difference vector from global to local coordinate system.
		rotation_matrix = np.array([[np.cos(orientation_target), -np.sin(orientation_target), 0.0],
		                            [np.sin(orientation_target), np.cos(orientation_target), 0.0], 
		                            [0.0, 0.0, 1.0]])
		pos_diff = rotation_matrix.T @ pos_diff
		
		return transfuser_t_u.algin_lidar(lidar, pos_diff, rot_diff)
	
	def _truncate_route_by_target_point(self, route_waypoints_np, target_point_np):
		"""
		Truncate route_pred based on target_point projection.
		
		Logic:
		- Project target_point onto the polyline formed by route_pred
		- If projection falls inside route_pred (route is truncated by target_point),
		  then the portion after projection is inaccurate and should not be used
		- If projection falls beyond route_pred's end, the entire route is valid
		
		Protection mechanism:
		- If truncated route has too few points (< MIN_POINTS_THRESHOLD) or
		  is too short (< MIN_LENGTH_THRESHOLD), skip truncation and use original route
		- This handles edge cases near the destination where target_point is very close
		
		Args:
			route_waypoints_np: (N, 2) numpy array in ego frame [x_forward, y_left]
			target_point_np: (2,) numpy array in ego frame [x_forward, y_left]
		
		Returns:
			truncated_route: (M, 2) numpy array, M <= N, the valid portion of route_pred
			truncation_idx: int, the index up to which the route is valid (-1 if no truncation)
		"""
		# Protection thresholds
		MIN_POINTS_THRESHOLD = 5  # Minimum number of points needed for reliable control
		MIN_LENGTH_THRESHOLD = 3.0  # Minimum route length in meters for reliable lookahead
		
		if len(route_waypoints_np) < 2:
			return route_waypoints_np, -1
		
		# Find the closest segment to target_point
		min_dist = float('inf')
		best_segment_idx = -1
		best_t = 0.0  # Parameter along segment [0, 1]
		best_proj_point = None
		
		for i in range(len(route_waypoints_np) - 1):
			p1 = route_waypoints_np[i]
			p2 = route_waypoints_np[i + 1]
			
			# Vector from p1 to p2
			v = p2 - p1
			# Vector from p1 to target_point
			w = target_point_np - p1
			
			# Length squared of segment
			l2 = np.dot(v, v)
			if l2 < 1e-10:  # Degenerate segment
				t = 0.0
				proj = p1
			else:
				# Project target_point onto the line containing the segment
				t = np.dot(w, v) / l2
				proj = p1 + t * v
			
			# Distance from target_point to projection
			dist = np.linalg.norm(target_point_np - proj)
			
			# We consider projections within or beyond the segment
			# t < 0: projection is before p1
			# 0 <= t <= 1: projection is within segment
			# t > 1: projection is beyond p2
			
			if dist < min_dist:
				min_dist = dist
				best_segment_idx = i
				best_t = t
				best_proj_point = proj
		
		# Determine truncation based on projection position
		# If best_t is within [0, 1], the projection is inside the route segment
		# If best_t > 1, check if we're on the last segment - if so, projection is beyond route
		
		if best_segment_idx == -1:
			# No valid segment found, return original route
			return route_waypoints_np, -1
		
		# Calculate the "arc length" position of the projection along the route
		# If projection is beyond the last point, no truncation needed
		is_on_last_segment = (best_segment_idx == len(route_waypoints_np) - 2)
		
		if best_t > 1.0 and is_on_last_segment:
			# Projection is beyond the end of route_pred
			# The entire route is valid for lookahead calculation
			return route_waypoints_np, -1
		
		# Projection is within route_pred or before it (shouldn't happen normally)
		# Truncate the route at the projection point
		if best_t <= 0.0:
			# Projection is at or before the start of this segment
			# Keep points up to and including segment start
			truncation_idx = best_segment_idx
		elif best_t >= 1.0:
			# Projection is at or beyond the end of this segment
			# Keep points up to and including segment end
			truncation_idx = best_segment_idx + 1
		else:
			# Projection is within the segment
			# Keep points up to segment start, then add the projection point
			truncation_idx = best_segment_idx
		
		# Build truncated route
		if truncation_idx >= len(route_waypoints_np) - 1:
			# No truncation needed
			return route_waypoints_np, -1
		
		# Include points up to truncation_idx, then add projection point
		truncated = route_waypoints_np[:truncation_idx + 1].copy()
		
		# Add the projection point if it's meaningfully different from the last included point
		if best_proj_point is not None and len(truncated) > 0:
			dist_to_last = np.linalg.norm(best_proj_point - truncated[-1])
			if dist_to_last > 0.1:  # Only add if more than 0.1m away
				truncated = np.vstack([truncated, best_proj_point])
		
		# ============ Protection mechanism ============
		# Calculate the total length of truncated route
		truncated_length = 0.0
		for i in range(len(truncated) - 1):
			truncated_length += np.linalg.norm(truncated[i + 1] - truncated[i])
		
		# Check if truncated route meets minimum requirements
		if len(truncated) < MIN_POINTS_THRESHOLD or truncated_length < MIN_LENGTH_THRESHOLD:
			# Truncated route is too short, skip truncation and use original route
			# This handles edge cases near destination where target_point is very close
			print(f"[Lateral] Skip truncation: points={len(truncated)}, length={truncated_length:.2f}m "
				  f"(thresholds: {MIN_POINTS_THRESHOLD} points, {MIN_LENGTH_THRESHOLD}m)")
			return route_waypoints_np, -1
		
		return truncated, truncation_idx
	
	def control_pid(self, route_waypoints, velocity, speed_waypoints, target_point=None):
		"""
		Predicts vehicle control with a PID controller.
		
		Args:
			route_waypoints: (1, N, 2) tensor in ego frame [x_forward, y_left]
			velocity: float, current speed in m/s
			speed_waypoints: (1, N, 2) tensor for speed calculation
			target_point: (1, 2) tensor in ego frame [x_forward, y_left], used for route truncation
		"""
		assert route_waypoints.size(0) == 1
		route_waypoints_np = route_waypoints[0].data.cpu().numpy()  # (N, 2)
		speed = velocity  # Already a float
		speed_waypoints_np = speed_waypoints[0].data.cpu().numpy()  # (N, 2)
		
		# Truncate route using target_point projection
		if target_point is not None:
			target_point_np = target_point[0].data.cpu().numpy()  # (2,)
			route_waypoints_np, truncation_idx = self._truncate_route_by_target_point(route_waypoints_np, target_point_np)
			if truncation_idx >= 0:
				print(f"[Lateral] Route truncated at index {truncation_idx}, remaining points: {len(route_waypoints_np)}")
		
		# MoT trajectory: 6 points, 0.5s interval each, total 3s
		# Point indices: 0(0.5s), 1(1.0s), 2(1.5s), 3(2.0s), 4(2.5s), 5(3.0s)
		mot_waypoint_interval = 0.5  # seconds between waypoints
		one_second_idx = 2 #1  # point[1] is at 1.0s
		half_second_idx = 0  # point[0] is at 0.5s
		
		if speed_waypoints_np.shape[0] >= 2:
			# Displacement from 0.5s to 1.0s position, multiply by 2 to get m/s
			# desired_speed = np.linalg.norm(speed_waypoints_np[one_second_idx] - speed_waypoints_np[half_second_idx]) * 2.0
			desired_speed = np.linalg.norm(speed_waypoints_np[one_second_idx] - speed_waypoints_np[half_second_idx])
		else:
			# Fallback: use first point distance, assuming it represents 0.5s travel
			desired_speed = np.linalg.norm(speed_waypoints_np[0]) * 2.0

		# Speed limit: cap desired_speed at 35 km/h = 35/3.6 ≈ 9.72 m/s
		# max_desired_speed_ms = 35.0 / 3.6  # 35 km/h in m/s
		# desired_speed = min(desired_speed, max_desired_speed_ms)

		brake = ((desired_speed < self.brake_speed) or ((speed / max(desired_speed, 1e-5)) > self.brake_ratio))
		
		delta = np.clip(desired_speed - speed, 0.0, self.clip_delta)
		throttle = self.speed_controller.step(delta)
		throttle = np.clip(throttle, 0.0, self.clip_throttle)
		throttle = throttle if not brake else 0.0
		

		route_interp = self.interpolate_waypoints(route_waypoints_np)
		
		
		steer = self.turn_controller.step(route_interp, speed)
		steer = np.clip(steer, -1.0, 1.0)
		steer = round(steer, 3)
		
		
		return steer, throttle, brake
	
	def interpolate_waypoints(self, waypoints):
		"""
		Interpolate waypoints to be 0.1m apart
		
		Args:
			waypoints: (N, 2) numpy array in ego frame [x_forward, y_left]
			
		Returns:
			interp_points: (M, 2) numpy array with points 0.1m apart
		"""
		waypoints = waypoints.copy()
		# Add origin point at the beginning
		waypoints = np.concatenate((np.zeros_like(waypoints[:1]), waypoints))
		shift = np.roll(waypoints, 1, axis=0)
		shift[0] = shift[1]
		
		dists = np.linalg.norm(waypoints - shift, axis=1)
		dists = np.cumsum(dists)
		dists += np.arange(0, len(dists)) * 1e-4  # Prevents dists not being strictly increasing
		
		interp = PchipInterpolator(dists, waypoints, axis=0)
		
		x = np.arange(0.1, dists[-1], 0.1)
		
		interp_points = interp(x)
		
		if interp_points.shape[0] == 0:
			interp_points = waypoints[None, -1]
		
		return interp_points
	
	@torch.no_grad()
	def run_step(self, input_data, timestamp):
		if not self.initialized:
			self._init()
		tick_data = self.tick(input_data)

		# Prepare current observations
		gt_velocity = torch.FloatTensor([tick_data['speed']]).to('cuda', dtype=torch.float32)
		# Use the same method as agent_simlingo.py: t_u.command_to_one_hot with self.commands[-2]
		one_hot_command = t_u.command_to_one_hot(self.commands[-2])
		cmd_one_hot = torch.from_numpy(one_hot_command[np.newaxis]).to('cuda', dtype=torch.float32)
		# Keep command variable for metadata (convert from 1-6 to 0-5 range)
		command = tick_data['next_command']
		if command < 0:
			command = 4
		command -= 1
		speed = torch.FloatTensor([float(tick_data['speed'])]).view(1,1).to('cuda', dtype=torch.float32)
		theta = torch.FloatTensor([float(tick_data['theta'])]).view(1,1).to('cuda', dtype=torch.float32)
		lidar = tick_data['lidar_bev'].to('cuda', dtype=torch.float32)
		
		rgb_front = torch.from_numpy(tick_data['rgb_front']).permute(2, 0, 1).float() / 255.0
		rgb_front = rgb_front.to('cuda', dtype=torch.float32)
		waypoint = torch.from_numpy(tick_data['gps']).float().to('cuda', dtype=torch.float32)
		target_point = torch.from_numpy(tick_data['target_point']).unsqueeze(0).float().to('cuda', dtype=torch.float32)
		next_target_point = torch.from_numpy(tick_data['next_target_point']).unsqueeze(0).float().to('cuda', dtype=torch.float32)
		
		# For debugging: print target point info occasionally
		if self.step % 20 == 0:
			tp = tick_data['target_point']
			ntp = tick_data['next_target_point']
			print(f"[Target Points] TP=({tp[0]:.1f},{tp[1]:.1f}), NTP=({ntp[0]:.1f},{ntp[1]:.1f})")

		# Accumulate observation history into buffers 
		self.lidar_bev_history.append(lidar)
		self.rgb_history.append(rgb_front)
		self.speed_history.append(speed)
		self.target_point_history.append(target_point)
		self.next_target_point_history.append(next_target_point)  # Both target_point and next_target_point are used for DP model
		self.next_command_history.append(cmd_one_hot)
		self.theta_history.append(theta)
		self.waypoint_history.append(waypoint)
		
		# Append throttle and brake from previous step (or 0 for first step)
		if self.step < 1:
			# First step: initialize with 0
			self.throttle_history.append(torch.tensor(0.0).view(1, 1).to('cuda'))
			self.brake_history.append(torch.tensor(0.0).view(1, 1).to('cuda'))
		else:
			# Use control from previous step
			prev_control = self.prev_control if self.prev_control is not None else carla.VehicleControl()
			self.throttle_history.append(torch.tensor(prev_control.throttle).view(1, 1).to('cuda'))
			self.brake_history.append(torch.tensor(prev_control.brake).view(1, 1).to('cuda'))
		
		# Buffer size = 31 frames (for obs_horizon with 10x sampling)
		BUFFER_PHASE = 31     # Fill buffer to minimum required size
		
		if self.step < BUFFER_PHASE:
			# Warmup phase: use previous control or default
			control = self.prev_control
			self.pid_metadata = {}
			self.pid_metadata['agent'] = 'warmup_phase'
			self.pid_metadata['step'] = self.step
		else:
			# Build observation dict
			# Both target_point and next_target_point are used for DP model in ego_status_stacked
			lidar_stacked, ego_status_stacked, rgb_stacked = self._build_obs_dict(
				tick_data, lidar, rgb_front, speed, theta, target_point, next_target_point,
				cmd_one_hot, waypoint
			)
			
			rgb_pil_list = []
			for i in range(rgb_stacked.shape[1]): 
				rgb_tensor = rgb_stacked[0, i]  # (C, H, W)
				rgb_np = (rgb_tensor.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
				rgb_pil = Image.fromarray(rgb_np, mode='RGB')
				rgb_pil_list.append(rgb_pil)
			
			# lidar_stacked shape: (1, obs_horizon, C, H, W)
			lidar_tensor = lidar_stacked[0, -1]  # (C, H, W) - last frame
			lidar_np = (lidar_tensor.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
			lidar_pil = Image.fromarray(lidar_np, mode='RGB')
			lidar_pil_list = [lidar_pil]  
			
			if self.stuck_helper > 0:
				# When stuck, always use next_target_point (farther) to help get unstuck
				target_point_speed=torch.cat([speed, next_target_point], dim=-1)
				print("Get stucked! Trigger the stuck helper!")
			else:
				target_point_speed=torch.cat([speed, target_point], dim=-1)  # (1, 3)

			prompt_cleaned, understanding_output, reasoning_output = build_cleaned_prompt_and_modes(target_point_speed)

			predicted_answer = self.inferencer(
				image=rgb_pil_list,  
				front=[rgb_pil_list[-1]],  
				lidar=lidar_pil_list,
				v_target_point=target_point_speed,
				text=prompt_cleaned,
				understanding_output=understanding_output,
				reasoning_output=reasoning_output,
				max_think_token_n=self.inference_args.max_num_tokens,
				do_sample=False,
				text_temperature=0.0,
			)

			pred_traj = predicted_answer['traj']  # Shape: (1, 6, 2) in ego frame [x_forward, y_left]
			pred_decision = predicted_answer['text']
			
			self.last_pred_traj = pred_traj.squeeze(0).float().cpu().numpy()  # (6, 2) in [x, y] format
			self.last_target_point = target_point.squeeze(0).float().cpu().numpy()  # (2,) in [x, y] format
			self.last_next_target_point = next_target_point.squeeze(0).float().cpu().numpy()  # (2,) in [x, y] format
		
			# DP trajectory refinement
			# Add batch dimension to features: (seq_len, feat_dim) -> (1, seq_len, feat_dim)
			dp_vit_feat = predicted_answer['dp_vit_feat']
			if dp_vit_feat.dim() == 2:
				dp_vit_feat = dp_vit_feat.unsqueeze(0)  # (1, Nvit, C)
			
			reason_feat = predicted_answer['reasoning_feat']
			if reason_feat.dim() == 2:
				reason_feat = reason_feat.unsqueeze(0)  # (1, Nr, C)
			
			# ========== Run TransFuser backbone to get BEV features ==========
			with torch.no_grad():
				# Convert inputs to bfloat16 to match transfuser backbone
				transfuser_rgb_bf16 = tick_data['transfuser_rgb'].to(torch.bfloat16)
				transfuser_lidar_bev_bf16 = tick_data['transfuser_lidar_bev'].to(torch.bfloat16)
				transfuser_output = self.transfuser_backbone(
					rgb=transfuser_rgb_bf16,  # (1, 3, H, W) on GPU, bfloat16
					lidar_bev=transfuser_lidar_bev_bf16  # (1, C, H, W) on GPU, bfloat16
				)
			
			# Extract transfuser features (following DiffusionDriveV2: only 2 features)
			# bev_feature: (B, 1512, 8, 8) - original BEV feature (x4)
			# bev_feature_upscale: (B, 64, 64, 64) - upsampled BEV (p3)
			transfuser_bev_feature = transfuser_output['bev_feature']  # (1, 1512, 8, 8)
			transfuser_bev_feature_upsample = transfuser_output['bev_feature_upscale']  # (1, 64, 64, 64)
			
			# Build dp_obs_dict with transfuser features
			dp_obs_dict = {
				'ego_status': ego_status_stacked,
				# TransFuser features (single frame, following DiffusionDriveV2)
				'transfuser_bev_feature': transfuser_bev_feature,  # (B, 1512, 8, 8)
				'transfuser_bev_feature_upsample': transfuser_bev_feature_upsample,  # (B, 64, 64, 64)
				# Reasoning tokens from MoT
				'reasoning_query_tokens': reason_feat[:, :7, :],  # (1, 7, C)
				'anchor': predicted_answer['traj']  # Pass anchor for truncated diffusion
			}
			dp_pred_traj = self.net.predict_action(dp_obs_dict)
			# self.last_dp_pred_traj = dp_pred_traj['action'].squeeze(0).copy()  # (6, 2) in [x, y] format
			# print("dp_pred_traj:", dp_pred_traj)

			# ================== control_pid method ==================
			# Following agent_simlingo convention:
			# - speed_waypoints: use pred_traj from MoT model for speed control (throttle/brake)
			# - route_waypoints: use 'route_pred' from DP model for lateral angle control (steering)
			# speed_waypoints = pred_traj.float()  # (1, 6, 2) for speed control - use MoT prediction
			speed_waypoints = torch.from_numpy(dp_pred_traj['action']).float() # - use DP prediction
			# Get DP prediction for route_pred
			self.last_dp_pred_traj = dp_pred_traj['action'].squeeze(0).copy()  # (6, 2) in [x, y] format
			
			# route_pred is 20 waypoints with equal intervals for lateral control
			route_pred = dp_pred_traj['route_pred']  # tensor (B, 20, 2)
			if isinstance(route_pred, torch.Tensor):
				route_waypoints = route_pred.float().cpu()  # (1, 20, 2) for steering control
			else:
				route_waypoints = torch.from_numpy(route_pred).float()
			self.last_route_pred = route_waypoints.squeeze(0).numpy().copy()  # (20, 2) for visualization

			
			gt_velocity = tick_data['speed']
			
			# Use target_point to truncate route for lateral control
			steer, throttle, brake = self.control_pid(route_waypoints, gt_velocity, speed_waypoints, target_point=target_point)
			
			# Restart mechanism in case the car got stuck (following simlingo logic)
			# 0.1 is just an arbitrary low number to threshold when the car is stopped
			if gt_velocity < 0.1:
				self.stuck_detector += 1
			
			elif gt_velocity >= 1.0:
				self.stuck_detector = 0
			
			# If stuck for too long, trigger force_move
			if self.stuck_detector > self.stuck_threshold:
				self.force_move = self.creep_duration
			
			# Force move: override throttle and brake to get unstuck
			if self.force_move > 0:
				throttle = max(self.creep_throttle, throttle)
				brake = False
				self.force_move -= 1
				print(f"force_move: {self.force_move}")

			print(f"stuck_detector: {self.stuck_detector}")

			
			control = carla.VehicleControl()
			control.steer = float(steer)
			control.throttle = float(throttle)
			control.brake = float(brake)
			
			# Speed limit enforcement: if current speed > 35 km/h, force brake
			# gt_velocity is in m/s, convert to km/h by multiplying 3.6
			if gt_velocity * 3.6 > 35:
				control.throttle = 0.0
				control.brake = 1.0
				print(f"[Speed Limit] Speed {gt_velocity * 3.6:.2f} km/h > 35 km/h, forcing brake!")
			
			# Store metadata
			self.pid_metadata = {
				'agent': 'mot',
				'steer': control.steer,
				'throttle': control.throttle,
				'brake': control.brake,
				'speed': gt_velocity,
				'command': command,
			}

			self.prev_control = control
			self.control = control  # Update control for UKF prediction in next tick
			metric_info = self.get_metric_info()
			self.metric_info[self.step] = metric_info

			if SAVE_PATH is not None:
				self.save(tick_data)

			##### Rendering ####
			ego_car_map = render_self_car(
				loc=np.array([0, 0]),
				ori=np.array([0, -1]),
				box=np.array([2.45, 1.0]),
				color=[1, 1, 0], pixels_per_meter=10, max_distance=30,
			)

			# Prepare trajectory for rendering (pred_traj - green)
			traj_for_render = pred_traj.squeeze(0).cpu().float().numpy().copy()  # (6, 2)
			traj_for_render[:, 1] = -traj_for_render[:, 1]  # Negate y: left -> right
			tp_for_render = target_point.cpu().float().numpy().copy()
			if tp_for_render.ndim == 2:
				tp_for_render = tp_for_render.squeeze(0)
			tp_for_render[1] = -tp_for_render[1]  # Negate y: left -> right
			

			trajectory = np.concatenate((traj_for_render, tp_for_render.reshape(1, 2)), axis=0)
			trajectory = trajectory[:, [1, 0]]
			trajectory[:, 0] = -trajectory[:, 0]  # y (now in col 0) 
			trajectory[:, 1] = -trajectory[:, 1]  # x (now in col 1)
			render_trajectory = render_waypoints(trajectory, pixels_per_meter=30, max_distance=20, color=(0, 255, 0))
			
			# Prepare dp_pred_traj for rendering (red)
			dp_traj_for_render = dp_pred_traj['action'].squeeze(0).copy()  # (6, 2) - already numpy
			dp_traj_for_render[:, 1] = -dp_traj_for_render[:, 1]  # Negate y: left -> right
			dp_trajectory = np.concatenate((dp_traj_for_render, tp_for_render.reshape(1, 2)), axis=0)
			dp_trajectory = dp_trajectory[:, [1, 0]]
			dp_trajectory[:, 0] = -dp_trajectory[:, 0]  # y (now in col 0) 
			dp_trajectory[:, 1] = -dp_trajectory[:, 1]  # x (now in col 1)
			render_dp_trajectory = render_waypoints(dp_trajectory, pixels_per_meter=30, max_distance=20, color=(255, 0, 0))

			ego_car_map = cv2.resize(ego_car_map, (200, 200))
			render_trajectory = cv2.resize(render_trajectory, (200, 200))
			render_dp_trajectory = cv2.resize(render_dp_trajectory, (200, 200))

			surround_map = np.clip(
				(
					ego_car_map.astype(np.float32)
					+ render_trajectory.astype(np.float32)
					+ render_dp_trajectory.astype(np.float32)
				),
				0,
				255,
			).astype(np.uint8)
			tick_data["predicted_trajectory"] = surround_map
			decision_1s, decision_2s, decision_3s = parse_decision_sequence(pred_decision)
			tick_data["decision_1s"] = decision_1s
			tick_data["decision_2s"] = decision_2s
			tick_data["decision_3s"] = decision_3s

			tick_data["rgb_raw"] = tick_data["rgb_front"]

			tick_data["rgb"] = cv2.resize(tick_data["rgb_front"], (800, 600))
			tick_data["bev_traj"] = cv2.resize(tick_data["bev_traj"], (400, 400))

			tick_data["control"] = "throttle: %.2f, steer: %.2f, brake: %.2f" % (
				control.throttle,
				control.steer,
				control.brake,
			)
			tick_data["speed"] = "speed: %.2f Km/h, target point x: %.2f m, target point y: %.2f m" % (gt_velocity*3.6, target_point.squeeze(0).cpu().float().numpy()[0], target_point.squeeze(0).cpu().float().numpy()[1])
			
			sentence1, sentence2 = split_prompt(prompt_cleaned)
			tick_data["language_1"] = "Instruction: " + sentence1
			tick_data["language_2"] = sentence2

			tick_data["mes"] = "speed: %.2f" % gt_velocity
			tick_data["time"] = "time: %.3f" % timestamp

			surface = self._hic.run_interface(tick_data)
			tick_data["surface"] = surface

		return control

	def save(self, tick_data):
		frame = self.step 
		Image.fromarray(tick_data['rgb_front']).save(self.save_path / 'rgb_front' / ('%04d.png' % frame))
		
		# Draw trajectory on BEV image if available
		bev_img = tick_data['bev'].copy()
		if self.last_pred_traj is not None:
			# Pass last_route_pred for visualization (20 waypoints for lateral control, blue points)
			# Pass both target_point and next_target_point for visualization
			bev_img = self._draw_trajectory_on_bev(bev_img, self.last_pred_traj, self.last_target_point, 
			                                        self.last_next_target_point, self.last_dp_pred_traj, self.last_route_pred)
		tick_data['bev_traj'] = bev_img
		Image.fromarray(bev_img).save(self.save_path / 'bev' / ('%04d.png' % frame))
		
		if 'lidar_bev' in tick_data:
			lidar_bev_tensor = tick_data['lidar_bev']
			if isinstance(lidar_bev_tensor, torch.Tensor):
				lidar_bev_tensor = lidar_bev_tensor.cpu().numpy()
			lidar_bev_img = (lidar_bev_tensor.transpose(1, 2, 0) * 255).astype(np.uint8)
			imageio.imwrite(str(self.save_path / 'lidar_bev' / (f'{frame:04d}.png')), lidar_bev_img)

		outfile = open(self.save_path / 'meta' / ('%04d.json' % frame), 'w')
		json.dump(self.pid_metadata, outfile, indent=4)
		outfile.close()

		# metric info
		outfile = open(self.save_path / 'metric_info.json', 'w')
		json.dump(self.metric_info, outfile, indent=4)
		outfile.close()

	def _draw_trajectory_on_bev(self, bev_img, traj, target_point=None, next_target_point=None, dp_traj=None, route_pred=None):
		"""
		Draw predicted trajectory on BEV image.
		
		BEV camera parameters:
		- Position: x=0, y=0, z=50 (50m height, looking down)
		- FOV: 50 degrees
		- Image size: 512x512
		
		Trajectory is in ego frame: [x, y] where x is forward, y is left (model convention)
		For BEV visualization, we negate y to convert to right-positive convention.
		BEV image: center is ego position, up is forward (negative x in image coords)
		
		Args:
			bev_img: numpy array (512, 512, 3) RGB image
			traj: numpy array (6, 2) trajectory points in ego frame [x_forward, y_left]
			target_point: numpy array (2,) target point in ego frame [x_forward, y_left], optional
			next_target_point: numpy array (2,) next target point in ego frame [x_forward, y_left], optional
			dp_traj: numpy array (6, 2) DP refined trajectory points in ego frame, optional
			route_pred: numpy array (20, 2) route waypoints for lateral control, optional
		
		Returns:
			bev_img: numpy array with trajectory drawn
		"""
		img_h, img_w = bev_img.shape[:2]  # 512, 512
		
		# BEV camera: z=50m, FOV=50 degrees
		# Calculate meters per pixel
		# FOV = 50 deg means the camera sees 50 degrees width/height
		# At z=50m, the ground coverage is: 2 * z * tan(FOV/2)
		fov_rad = np.deg2rad(50.0)
		ground_size = 2 * 50.0 * np.tan(fov_rad / 2)  # meters covered by the image
		meters_per_pixel = ground_size / img_w  # ~0.093 m/pixel
		
		# Image center is ego position
		cx, cy = img_w // 2, img_h // 2
		
		# Convert trajectory points to pixel coordinates
		# Model ego frame: x is forward, y is LEFT (positive y = left)
		# BEV image: center is ego, up (-row) is forward, right (+col) is right
		# Need to negate y to convert from left-positive to right-positive
		# So: pixel_col = cx + y / meters_per_pixel (negate y: left -> right, then right is +col)
		#     pixel_row = cy - x / meters_per_pixel (x forward -> -row, i.e., up)
		
		pixels = []
		for i in range(len(traj)):
			x, y = traj[i]  # x: forward, y: left (model convention)
			# Negate y for visualization: left-positive -> right-positive
			pixel_col = int(cx + y / meters_per_pixel)  # y_left negated: +y_left -> -col, so use + to flip
			pixel_row = int(cy - x / meters_per_pixel)
			pixels.append((pixel_col, pixel_row))
		
		# Draw trajectory using cv2
		# Draw lines connecting waypoints
		for i in range(len(pixels) - 1):
			pt1 = pixels[i]
			pt2 = pixels[i + 1]
			# Check if points are within image bounds
			if (0 <= pt1[0] < img_w and 0 <= pt1[1] < img_h and
				0 <= pt2[0] < img_w and 0 <= pt2[1] < img_h):
				cv2.line(bev_img, pt1, pt2, (0, 255, 0), 2)  # Green line
		
		# Draw waypoints as circles
		for i, (col, row) in enumerate(pixels):
			if 0 <= col < img_w and 0 <= row < img_h:
				# Color gradient: start (red) -> end (blue)
				color_r = int(255 * (1 - i / (len(pixels) - 1)))
				color_b = int(255 * (i / (len(pixels) - 1)))
				cv2.circle(bev_img, (col, row), 5, (color_r, 0, color_b), -1)
		
		# Draw DP trajectory if provided (red color)
		if dp_traj is not None:
			dp_pixels = []
			for i in range(len(dp_traj)):
				x, y = dp_traj[i]  # x: forward, y: left (model convention)
				pixel_col = int(cx + y / meters_per_pixel)
				pixel_row = int(cy - x / meters_per_pixel)
				dp_pixels.append((pixel_col, pixel_row))
			
			# Draw DP trajectory lines (red)
			for i in range(len(dp_pixels) - 1):
				pt1 = dp_pixels[i]
				pt2 = dp_pixels[i + 1]
				if (0 <= pt1[0] < img_w and 0 <= pt1[1] < img_h and
					0 <= pt2[0] < img_w and 0 <= pt2[1] < img_h):
					cv2.line(bev_img, pt1, pt2, (255, 0, 0), 2)  # Red line
			
			# Draw DP waypoints as circles (red with gradient to orange)
			for i, (col, row) in enumerate(dp_pixels):
				if 0 <= col < img_w and 0 <= row < img_h:
					# Color gradient: start (red) -> end (orange)
					color_g = int(128 * (i / (len(dp_pixels) - 1))) if len(dp_pixels) > 1 else 0
					cv2.circle(bev_img, (col, row), 4, (255, color_g, 0), -1)
		
		# Draw route_pred waypoints if provided (blue color) - used for lateral/steering control
		if route_pred is not None:
			route_pixels = []
			for i in range(len(route_pred)):
				x, y = route_pred[i]  # x: forward, y: left (model convention)
				pixel_col = int(cx + y / meters_per_pixel)
				pixel_row = int(cy - x / meters_per_pixel)
				route_pixels.append((pixel_col, pixel_row))
			
			# Draw route_pred trajectory lines (blue)
			for i in range(len(route_pixels) - 1):
				pt1 = route_pixels[i]
				pt2 = route_pixels[i + 1]
				if (0 <= pt1[0] < img_w and 0 <= pt1[1] < img_h and
					0 <= pt2[0] < img_w and 0 <= pt2[1] < img_h):
					cv2.line(bev_img, pt1, pt2, (255, 165, 0), 1)  # Orange line (thinner)
			
			# Draw route_pred waypoints as circles (blue)
			for i, (col, row) in enumerate(route_pixels):
				if 0 <= col < img_w and 0 <= row < img_h:
					# Solid blue points for route_pred
					cv2.circle(bev_img, (col, row), 3, (0, 0, 255), -1)  # Blue circles (smaller)
		
		# Draw target point if provided (cyan/aqua color with larger circle)
		if target_point is not None:
			x, y = target_point[0], target_point[1]  # x: forward, y: left (model convention)
			# Negate y for visualization
			tp_col = int(cx + y / meters_per_pixel)  # Negate y: +y_left -> -col, use + to flip
			tp_row = int(cy - x / meters_per_pixel)
			if 0 <= tp_col < img_w and 0 <= tp_row < img_h:
				cv2.circle(bev_img, (tp_col, tp_row), 10, (0, 255, 255), -1)  # Cyan circle for target point
				cv2.circle(bev_img, (tp_col, tp_row), 12, (255, 255, 255), 2)  # White border
		
		# Draw next target point if provided (magenta/pink color with larger circle)
		if next_target_point is not None:
			x, y = next_target_point[0], next_target_point[1]  # x: forward, y: left (model convention)
			# Negate y for visualization
			ntp_col = int(cx + y / meters_per_pixel)
			ntp_row = int(cy - x / meters_per_pixel)
			if 0 <= ntp_col < img_w and 0 <= ntp_row < img_h:
				cv2.circle(bev_img, (ntp_col, ntp_row), 10, (255, 0, 255), -1)  # Magenta circle for next target point
				cv2.circle(bev_img, (ntp_col, ntp_row), 12, (255, 255, 255), 2)  # White border
		
		# Draw ego position (center)
		cv2.circle(bev_img, (cx, cy), 8, (255, 255, 0), -1)  # Yellow circle for ego
		
		return bev_img

	def destroy(self):
		del self.net
		torch.cuda.empty_cache()

	def gps_to_location(self, gps):
		# gps content: numpy array: [lat, lon, alt]
		lat, lon = gps
		scale = math.cos(self.lat_ref * math.pi / 180.0)
		my = math.log(math.tan((lat+90) * math.pi / 360.0)) * (EARTH_RADIUS_EQUA * scale)
		mx = (lon * (math.pi * EARTH_RADIUS_EQUA * scale)) / 180.0
		y = scale * EARTH_RADIUS_EQUA * math.log(math.tan((90.0 + self.lat_ref) * math.pi / 360.0)) - my
		x = mx - scale * self.lon_ref * math.pi * EARTH_RADIUS_EQUA / 180.0
		return np.array([x, y])









